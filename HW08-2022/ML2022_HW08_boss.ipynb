{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["bDk9r2YOcDc9","Oi12tJMYWi0Q","DCgNXSsEWuY7","HNe7QU7n7cqh","6X6fkGPnYyaF","1EbfwRREhA7c","vrJ9bScg9AgO","XKNUImqUhIeq"]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":35684,"databundleVersionId":3471431,"sourceType":"competition"}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Homework 8 - Anomaly Detection**\n\nIf there are any questions, please contact mlta-2022spring-ta@googlegroups.com\n\nSlide:    [Link]()　Kaggle: [Link](https://www.kaggle.com/c/ml2022spring-hw8)    \n      \n**Baseline**\n- Simple\n    * Sample code\n- Medium    0.72895\n    * Adjust model structure\n- Strong    0.77196\n    * Multi-encoder autoencoder\n- Boss      0.79506\n    * Add random noise and an extra classifier\n    * [Papers of anomaly detection](https://github.com/hoya012/awesome-anomaly-detection#anomaly-classification-target)","metadata":{"id":"YiVfKn-6tXz8"}},{"cell_type":"markdown","source":"# Set up the environment\n","metadata":{"id":"bDk9r2YOcDc9"}},{"cell_type":"markdown","source":"## Package installation","metadata":{"id":"Oi12tJMYWi0Q"}},{"cell_type":"code","source":"# Training progress bar\n!pip install -q qqdm","metadata":{"id":"7LexxyPWWjJB","outputId":"3a733a84-fca3-4e7c-fb9b-bfd5f890114d","execution":{"iopub.status.busy":"2024-09-04T07:25:56.291619Z","iopub.execute_input":"2024-09-04T07:25:56.292044Z","iopub.status.idle":"2024-09-04T07:26:14.339953Z","shell.execute_reply.started":"2024-09-04T07:25:56.292006Z","shell.execute_reply":"2024-09-04T07:26:14.338549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Downloading data","metadata":{"id":"DCgNXSsEWuY7"}},{"cell_type":"markdown","source":"- Training data\n    * 100000 human faces\n- Testing data\n    * About 10000 from the same distribution with training data (label 0)\n    * About 10000 from another distribution (anomalies, label 1)\n- Format\n    * data/          \n     |----- trainingset.npy          \n     |----- testingset.npy\n    * Shape: (#images, 64, 64, 3) for each .npy file\n        - training data(100000, 64, 64, 3)\n        - testing data(19636, 64, 64, 3)\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-09-04T07:26:14.342645Z","iopub.execute_input":"2024-09-04T07:26:14.343108Z","iopub.status.idle":"2024-09-04T07:26:14.714175Z","shell.execute_reply.started":"2024-09-04T07:26:14.343065Z","shell.execute_reply":"2024-09-04T07:26:14.713067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !wget https://github.com/MachineLearningHW/HW8_Dataset/releases/download/v1.0.0/data.zip","metadata":{"id":"SCLJtgF2BLSK","outputId":"54d462c4-2121-46a9-a966-1ca9b01e7b61","execution":{"iopub.status.busy":"2024-09-04T07:26:14.715612Z","iopub.execute_input":"2024-09-04T07:26:14.716357Z","iopub.status.idle":"2024-09-04T07:26:14.721451Z","shell.execute_reply.started":"2024-09-04T07:26:14.716320Z","shell.execute_reply":"2024-09-04T07:26:14.720214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !unzip data.zip","metadata":{"id":"0K5kmlkuWzhJ","outputId":"c12176a4-f513-4ed3-c351-c82ef26e8072","execution":{"iopub.status.busy":"2024-09-04T07:26:14.724819Z","iopub.execute_input":"2024-09-04T07:26:14.725206Z","iopub.status.idle":"2024-09-04T07:26:14.731497Z","shell.execute_reply.started":"2024-09-04T07:26:14.725166Z","shell.execute_reply":"2024-09-04T07:26:14.730527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import packages","metadata":{"id":"HNe7QU7n7cqh"}},{"cell_type":"code","source":"import random\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision.models as models\nfrom torch.optim import Adam, AdamW\nfrom qqdm import qqdm, format_str\nimport pandas as pd","metadata":{"id":"Jk3qFK_a7k8P","execution":{"iopub.status.busy":"2024-09-04T07:26:14.732904Z","iopub.execute_input":"2024-09-04T07:26:14.733426Z","iopub.status.idle":"2024-09-04T07:26:19.961749Z","shell.execute_reply.started":"2024-09-04T07:26:14.733389Z","shell.execute_reply":"2024-09-04T07:26:19.960662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data","metadata":{"id":"6X6fkGPnYyaF"}},{"cell_type":"code","source":"train = np.load('/kaggle/input/ml2022spring-hw8/data/trainingset.npy', allow_pickle=True)  # allow_pickle是序列化和反序列化的一个模块，将复杂的对象转换为字节流，以便存储或传输，然后再将字节流还原为对象\ntest = np.load('/kaggle/input/ml2022spring-hw8/data/testingset.npy', allow_pickle=True)\n\nprint(train.shape)\nprint(test.shape)","metadata":{"id":"k7Wd4yiUYzAm","outputId":"2c2fa7bf-1c0a-4090-ac16-627c1fe5ca5c","execution":{"iopub.status.busy":"2024-09-04T07:26:19.963106Z","iopub.execute_input":"2024-09-04T07:26:19.963910Z","iopub.status.idle":"2024-09-04T07:26:37.260364Z","shell.execute_reply.started":"2024-09-04T07:26:19.963877Z","shell.execute_reply":"2024-09-04T07:26:37.259171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random seed\nSet the random seed to a certain value for reproducibility.","metadata":{"id":"_flpmj6OYIa6"}},{"cell_type":"code","source":"def same_seeds(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nsame_seeds(48763)","metadata":{"id":"Gb-dgXQYYI2Q","execution":{"iopub.status.busy":"2024-09-04T07:26:37.261415Z","iopub.execute_input":"2024-09-04T07:26:37.261721Z","iopub.status.idle":"2024-09-04T07:26:37.270512Z","shell.execute_reply.started":"2024-09-04T07:26:37.261696Z","shell.execute_reply":"2024-09-04T07:26:37.269234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Autoencoder","metadata":{"id":"zR9zC0_Df-CR"}},{"cell_type":"markdown","source":"# Models & loss","metadata":{"id":"1EbfwRREhA7c"}},{"cell_type":"markdown","source":"fcn 为避免后续要对输出再操作改变形状，可以直接再decoder中写   \n```python      \nnn.Linear(1024, 3 * 64 * 64),\n            nn.Unflatten(1, (3, 64, 64)),\n            nn.Tanh()\n        ```        \n或者在forward中改变形状    \n```python       \ndef forward(self, x):\n        x = x.view(x.size(0), -1)  # 展平输入数据\n        x = self.encoder(x)\n        x = self.decoder(x)\n        x = x.view(x.size(0), 3, 64, 64)  # 重塑输出数据\n        return x\n    ```","metadata":{}},{"cell_type":"markdown","source":"### fully-connected encoder","metadata":{}},{"cell_type":"code","source":"class fcn_autoencoder(nn.Module):\n    def __init__(self):\n        super(fcn_autoencoder, self).__init__()\n        # Medium\n        self.encoder = nn.Sequential(\n            nn.Linear(64 * 64 * 3, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(), \n            nn.Linear(256, 128), \n            nn.ReLU(), \n            nn.Linear(128, 64)\n        )\n        \n        self.decoder = nn.Sequential(\n            nn.Linear(64, 128),\n            nn.ReLU(), \n            nn.Linear(128, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.ReLU(), \n            nn.Linear(1024, 64 * 64 * 3), \n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","metadata":{"id":"Wi8ds1fugCkR","execution":{"iopub.status.busy":"2024-09-04T07:26:37.271943Z","iopub.execute_input":"2024-09-04T07:26:37.272312Z","iopub.status.idle":"2024-09-04T07:26:37.298158Z","shell.execute_reply.started":"2024-09-04T07:26:37.272284Z","shell.execute_reply":"2024-09-04T07:26:37.297143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conv Encoder","metadata":{}},{"cell_type":"code","source":"class conv_autoencoder(nn.Module):\n    def __init__(self):\n        super(conv_autoencoder, self).__init__()\n        # Medium\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 12, 4, stride=2, padding=1),\n            nn.BatchNorm2d(12),\n            nn.ReLU(),\n            nn.Conv2d(12, 24, 4, stride=2, padding=1),\n            nn.BatchNorm2d(24),\n            nn.ReLU(),\n            nn.Conv2d(24, 48, 4, stride=2, padding=1),\n            nn.BatchNorm2d(48),\n            nn.ReLU(),\n            nn.Conv2d(48, 96, 4, stride=2, padding=1),\n            nn.BatchNorm2d(96),\n            nn.ReLU(),\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(96, 48, 4, stride=2, padding=1),\n            nn.BatchNorm2d(48),\n            nn.ReLU(),\n            nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),\n            nn.BatchNorm2d(24),\n            nn.ReLU(),\n            nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1),\n            nn.BatchNorm2d(12),\n            nn.ReLU(),\n            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x     ","metadata":{"id":"Wi8ds1fugCkR","execution":{"iopub.status.busy":"2024-09-04T07:26:37.299645Z","iopub.execute_input":"2024-09-04T07:26:37.300073Z","iopub.status.idle":"2024-09-04T07:26:37.313043Z","shell.execute_reply.started":"2024-09-04T07:26:37.300036Z","shell.execute_reply":"2024-09-04T07:26:37.311710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### VAE","metadata":{}},{"cell_type":"code","source":"   \nclass VAE(nn.Module):\n    def __init__(self):\n        super(VAE, self).__init__()\n        \n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 12, 4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(12, 24, 4, stride=2, padding=1),\n            nn.ReLU(),\n        )\n        self.enc_out_1 = nn.Sequential(\n            nn.Conv2d(24, 48, 4, stride=2, padding=1),\n            nn.ReLU(),\n        )\n        self.enc_out_2 = nn.Sequential(\n            nn.Conv2d(24, 48, 4, stride=2, padding=1),\n            nn.ReLU(),\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),\n            nn.Tanh(),\n        )\n\n    # 通过encoder将图像编码为中间表示h1, 再通过enc_out_1和enc_out_2分别输出均值 mu和方差的对数 logvar\n    def encode(self, x):\n        h1 = self.encoder(x)\n        return self.enc_out_1(h1), self.enc_out_2(h1)\n\n    # 通过重参数化技巧从以 mu 为中心，标准差为 std 的正态分布中采样潜在变量 z\n    def reparametrize(self, mu, logvar):\n        std = logvar.mul(0.5).exp_()\n        if torch.cuda.is_available():\n            eps = torch.cuda.FloatTensor(std.size()).normal_()\n        else:\n            eps = torch.FloatTensor(std.size()).normal_() # 创建一个与 std 形状相同的张量，并从标准正态分布（均值为0，标准差为1）中随机采样。\n        eps = torch.Tensor(eps)\n        return eps.mul(std).add_(mu) # 将随机噪声 eps 乘以标准差 std，加上均值 mu, 得到从以 mu 为中心，标准差为 std 的正态分布中采样得到潜在变量 z\n\n    def decode(self, z):\n        return self.decoder(z)\n\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparametrize(mu, logvar)\n        return self.decode(z), mu, logvar\n\n\ndef loss_vae(recon_x, x, mu, logvar, criterion):\n    \"\"\"\n    recon_x: generating images\n    x: origin images\n    mu: latent mean\n    logvar: latent log variance\n    \"\"\"\n    mse = criterion(recon_x, x)\n    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n    KLD = torch.sum(KLD_element).mul_(-0.5)\n    return mse + KLD","metadata":{"id":"Wi8ds1fugCkR","execution":{"iopub.status.busy":"2024-09-04T07:26:37.317651Z","iopub.execute_input":"2024-09-04T07:26:37.317992Z","iopub.status.idle":"2024-09-04T07:26:37.332705Z","shell.execute_reply.started":"2024-09-04T07:26:37.317966Z","shell.execute_reply":"2024-09-04T07:26:37.331593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ResNet","metadata":{}},{"cell_type":"markdown","source":"```markdown\nResnet\nencoder\n\npreconv 3,64,64 -> 32,64,64\n\nResidual最后返回的是两层conv的输出+residual，有downsample时，residual=downsample out，无residual=整个block的input\nlayer0: make_residual \n\tResidual(32,64,stride=2)\n\t\tconv1: 32,64,64 -> 64,32,32\n\t\tconv2: 64,32,32 -> 64,32,32\n\t\tdownsample: 32,64,64 -> 64,32,32\n\tResidual(64,64)\n\t\tconv1: 64,32,32 -> 64,32,32\n\t\tconv2: 64,32,32 -> 64,32,32\n\tResidual(64,64)\n\t\tconv1: 64,32,32 -> 64,32,32\n\t\tconv2: 64,32,32 -> 64,32,32\n\nlayer1: make_residual\n\tResidual(64,128,stride=2)\n\t\tconv1: 64,32,32 -> 128,16,16\n\t\tconv2: 128,16,16 -> 128,16,16\n\t\tdownsample: 64,32,32 -> 128,16,16\n\tResidual(128,128)\n\t\tconv1: 128,16,16 -> 128,16,16\n\t\tconv2: 128,16,16 -> 128,16,16\n\nlayer2: make_residual\n\tResidual(128,128,stride=2)\n\t\tconv1: 128,16,16 -> 128,8,8\n\t\tconv2: 128,18,8 -> 128,8,8\n\t\tdownsample: 128,16,16 -> 128,8,8\n\tResidual(128,128)\n\t\tconv1: 128,8,8 -> 128,8,8\n\t\tconv2: 128,8,8 -> 128,8,8\n\nlayer3: make_residual\n\tResidual(128,64,stride=2)\n\t\tconv1: 128,8,8 -> 64,4,4\n\t\tconv2: 64,4,4 -> 64,4,4\n\t\tdownsample: 128,8,8 -> 64,4,4\n\tResidual(64,64)\n\t\tconv1: 64,4,4 -> 64,4,4\n\t\tconv2: 64,4,4 -> 64,4,4\n\nfc: full-connected\n\tflatten: 64,4,4 -> 64*4*4\n\tlinear: 64*4*4 -> 64\n\ndecoder:\n\n\tlinear: 64 -> 64*4*4\n\tunflatten: 64*4*4 -> 64,4,4\n\tconvtranspose: 64,4,4 -> 128,8,8\n\tconvtranspose: 128,8,8 -> 128,16,16\n\tconvtranspose: 128,16,16 -> 128,32,32\n\tconvtranspose: 128,32,32 -> 3,64,64\n\n```","metadata":{}},{"cell_type":"code","source":"class Residual_block(nn.Module):\n    def __init__(self, ic, oc, stride=1): # 当传入 stride = 2 时，会把图片长宽缩小一倍\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(ic, oc, stride=stride, padding=1, kernel_size=3), # stride=2: (H,W) -> (H/2, W/2)\n            nn.BatchNorm2d(oc),\n            nn.ReLU(inplace=True)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(oc, oc, stride=1, padding=1, kernel_size=3), # (H,W) -> (H,W)\n            nn.BatchNorm2d(oc)\n        )\n        \n        self.downsample = None # 让原来的 x 变成能和 forward(x) 相加的形状，包括 channel 和 (H,W) 都应相同\n        if((stride != 1)  or (ic != oc)): # stride != 1 -> (H,W) 变小 ic != oc -> channel 不同 则创建下采样路径\n            self.downsample = nn.Sequential(\n                nn.Conv2d(ic, oc, stride=stride, kernel_size=1),\n                nn.BatchNorm2d(oc)\n            )\n            \n    def forward(self, x):\n        residual = x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        \n        if(self.downsample != None):\n            residual = self.downsample(residual)\n        \n        x = x + residual\n        x = nn.ReLU(inplace = True)(x)\n        return x\n\nclass ResNet(nn.Module):\n    def __init__(self, block=Residual_block, num_layers = [2, 1, 1, 1]):\n        super().__init__()\n        self.preconv = nn.Sequential( # 3*64*64 --> 32*64*64\n            nn.Conv2d(3, 32, kernel_size=3, padding=1, stride=1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True)\n        )\n        \n        def make_residual(block, ic, oc, num_layer, stride=1):\n            layers = []\n            layers.append(block(ic, oc, stride))\n            for i in range(num_layer-1):\n                layers.append(block(oc, oc))\n            return nn.Sequential(*layers)\n\n        self.layer0 = make_residual(block, ic=32, oc=64, num_layer=num_layers[0], stride=2)\n        self.layer1 = make_residual(block, ic=64, oc=128, num_layer=num_layers[1], stride=2)\n        self.layer2 = make_residual(block, ic=128, oc=128, num_layer=num_layers[2], stride=2)\n        self.layer3 = make_residual(block, ic=128, oc=64, num_layer=num_layers[3], stride=2) \n\n        self.fc = nn.Sequential(\n            nn.Flatten(), # 也可以用 .view(shape[0], -1)\n            nn.Dropout(0.2),\n            nn.Linear(64*4*4, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(inplace = True)\n        )\n        \n        # 关于 ConvTranspose2d 的介绍：https://blog.csdn.net/qq_36201400/article/details/112604740\n        # 大概就是在原图相邻的两个格子之间插 stride-1 个 0（这样原图就会变大了），padding <- kernel-padding-1\n        self.decoder = nn.Sequential(\n            nn.Linear(64, 64*4*4), # (64) -> (64*4*4)\n            nn.BatchNorm1d(64*4*4),\n            nn.ReLU(),\n            nn.Unflatten(1, (64, 4, 4)), # (64*4*4) -> (64, 4, 4)\n            nn.ConvTranspose2d(64,128,kernel_size=4,stride=2,padding=1), # (64,4,4) -> (128,8,8)\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128,128,kernel_size=4,stride=2,padding=1), # (128,8,8) -> (128,16,16)\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128,128,kernel_size=4,stride=2,padding=1), # (128,16,16) -> (128,32,32)\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128,3,kernel_size=4,stride=2,padding=1), # (128,32,32) -> (3,64,64)\n            nn.Tanh()\n        )\n    \n    def encoder(self, x):\n        x = self.preconv(x) # (3,64,64) -> (32,64,64)\n        x = self.layer0(x) # (32,64,64) -> (64,32,32) 且通过 resnet(shortcut) 实现，下同\n        x = self.layer1(x) # 64*32*32 ->128*16*16\n        x = self.layer2(x) # 128*16*16->128*8*8\n        x = self.layer3(x) # (128,8,8) -> (64,4,4)\n        x = self.fc(x) # (64,4,4) -> (64*4*4) -> (64)\n        return x\n        \n    def forward(self, x): # x : (3, 64, 64)\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-09-04T07:26:37.334617Z","iopub.execute_input":"2024-09-04T07:26:37.335377Z","iopub.status.idle":"2024-09-04T07:26:37.356846Z","shell.execute_reply.started":"2024-09-04T07:26:37.335338Z","shell.execute_reply":"2024-09-04T07:26:37.355845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multi-encoder autoencoder","metadata":{}},{"cell_type":"code","source":"# 卷积编码器\nclass ConvEncoder(nn.Module):\n    def __init__(self, Split_channel = False):\n        super(ConvEncoder, self).__init__()\n        if Split_channel:\n            self.encoder = nn.Sequential(\n                nn.Conv2d(1, 12, kernel_size=3, stride=2, padding=1),\n                nn.ReLU(),\n                nn.Conv2d(12, 64, kernel_size=3, stride=2, padding=1),\n                nn.ReLU(),\n                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n                nn.ReLU()\n            )\n        else:\n            self.encoder = nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n                nn.ReLU(),\n                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n                nn.ReLU()\n            )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        return x\n\n# fcn编码器\nclass fcnEncoder(nn.Module):\n    def __init__(self, Split_channel = False):\n        super(fcnEncoder, self).__init__()\n        if Split_channel:\n            C = 1\n        else:\n            C = 3\n        self.encoder = nn.Sequential(\n            nn.Linear(64 * 64 * C, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(), \n            nn.Linear(256, 128), \n            nn.ReLU(), \n            nn.Linear(128, 32)\n        )\n    \n    def forward(self,x):\n        x = x.view(x.size(0), -1)  # Flatten the input\n        x = self.encoder(x)\n        return x\n    \nclass fcnDecoder(nn.Module):\n    def __init__(self, output_dim=3*64*64):\n        super(fcnDecoder, self).__init__()\n        self.output_dim = output_dim\n        self.linear1 = None\n        self.relu = nn.ReLU()\n        self.linear2 = None\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        # 在 forward 中动态创建线性层\n        if self.linear1 is None:\n            input_dim = x.size(1)\n            self.linear1 = nn.Linear(input_dim, 1024).to(x.device)\n            self.linear2 = nn.Linear(1024, self.output_dim).to(x.device)\n\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        x = self.tanh(x)\n        x = torch.reshape(x, (-1, 3, 64, 64))\n        return x\n\"\"\"\n# fcn解码器 \nclass fcnDecoder(nn.Module):\n    def __init__(self, input_dim, output_dim=3*64*64):\n        super(fcnDecoder, self).__init__()\n        self.decoder = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, output_dim),\n            nn.Tanh()\n        )\n    \n    def forward(self,x):\n        x = self.decoder(x)\n        return x\n    \"\"\"\nclass ConvDecoder(nn.Module):\n    def __init__(self):\n        super(ConvDecoder, self).__init__()\n        self.transition = nn.Conv2d(384, 512, kernel_size=3, stride=1, padding=1)\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n            nn.Tanh()\n        )\n            \n    def forward(self, x):\n        x = self.transition(x)    # Pass through the transition layer\n        x = self.decoder(x)       # Decode the combined features\n        return x\n    \nclass MultiEncoderAutoencoder(nn.Module):\n    def __init__(self, Split_channel = False, Multi_Type='fcn'):\n        super(MultiEncoderAutoencoder, self).__init__()\n        self.Split_channel = Split_channel\n        self.Multi_Type = Multi_Type\n        self.fcnencoder = fcnEncoder()\n        self.convencoder = ConvEncoder()\n        self.fcndecoder = fcnDecoder()\n        self.convdecoder = ConvDecoder()\n    \n    def forward(self, x):\n        if self.Split_channel:\n            input_1 = x[:, 0:1, :, :]\n            input_2 = x[:, 1:2, :, :]\n            input_3 = x[:, 2:3, :, :]\n        else:\n            input_1 = x\n            input_2 = x\n            input_3 = x\n    \n        if self.Multi_Type == 'fcn':\n            x1 = self.fcnencoder(input_1)\n            x2 = self.fcnencoder(input_2)\n            x3 = self.fcnencoder(input_3)\n            concatenated = torch.cat((x1,x2,x3), dim=-1)  # x1,x2,x3(batchsize, feature_nums) Concatenate the three encoder outputs along the feature dimension\n            # input_dim = concatenated.shape[1]\n            output = self.fcndecoder(concatenated)\n        \n        elif self.Multi_Type == 'cnn':\n            x1 = self.convencoder(input_1)\n            x2 = self.convencoder(input_2)\n            x3 = self.convencoder(input_3)\n            concatenated = torch.cat((x1, x2, x3), dim=1)  # x1,x2,x3(batchsize, channels, height, width) Concatenate the three encoder outputs along the channel dimension\n            output = self.convdecoder(concatenated)\n            \n        elif self.Multi_Type == 'mixed':\n            x1 = self.fcnencoder(input_1)\n            x2 = self.fcnencoder(input_2)\n            x3 = self.convencoder(input_3)\n            x3 = x3.view(x3.size(0), -1)  # Flatten the conv encoder output\n            concatenated = torch.cat((x1, x2, x3), dim=-1)  # 将多个编码器的输出连接起来\n            output = self.fcndecoder(concatenated)\n        else:\n            raise ValueError(\"Invalid Multi_Type. Choose 'fcn', 'conv', or 'mixed'.\")\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-09-04T07:55:48.290883Z","iopub.execute_input":"2024-09-04T07:55:48.291252Z","iopub.status.idle":"2024-09-04T07:55:48.318615Z","shell.execute_reply.started":"2024-09-04T07:55:48.291226Z","shell.execute_reply":"2024-09-04T07:55:48.317607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset module\n\nModule for obtaining and processing data. The transform function here normalizes image's pixels from [0, 255] to [-1.0, 1.0].\n","metadata":{"id":"vrJ9bScg9AgO"}},{"cell_type":"code","source":"class CustomTensorDataset(TensorDataset):\n    \"\"\"TensorDataset with support of transforms.\n    \"\"\"\n    def __init__(self, tensors):\n        self.tensors = tensors\n        # 输入张量的最后一个维度是3（通常表示RGB图像），则将张量的维度重新排列，从 (N, H, W, C) 转换为 (N, C, H, W)\n        if tensors.shape[-1] == 3:\n            self.tensors = tensors.permute(0, 3, 1, 2)\n\n        self.transform = transforms.Compose([\n          transforms.Lambda(lambda x: x.to(torch.float32)),\n          transforms.Lambda(lambda x: 2. * x/255. - 1.),    # 将图像像素值从 [0, 255] 映射到 [-1.0, 1.0]\n        ])\n\n    def __getitem__(self, index):\n        x = self.tensors[index]\n\n        if self.transform:\n            # mapping images to [-1.0, 1.0]\n            x = self.transform(x)\n\n        return x\n\n    # 返回数据集的大小，即张量的第一个维度的大小\n    def __len__(self):\n        return len(self.tensors)","metadata":{"id":"33fWhE-h9LPq","execution":{"iopub.status.busy":"2024-09-04T07:29:20.666376Z","iopub.execute_input":"2024-09-04T07:29:20.666795Z","iopub.status.idle":"2024-09-04T07:29:20.675911Z","shell.execute_reply.started":"2024-09-04T07:29:20.666767Z","shell.execute_reply":"2024-09-04T07:29:20.674681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"XKNUImqUhIeq"}},{"cell_type":"markdown","source":"## Configuration\n","metadata":{"id":"7ebAJdjFmS08"}},{"cell_type":"code","source":"print(torch.cuda.is_available())","metadata":{"execution":{"iopub.status.busy":"2024-09-04T07:31:35.940940Z","iopub.execute_input":"2024-09-04T07:31:35.941720Z","iopub.status.idle":"2024-09-04T07:31:35.947397Z","shell.execute_reply.started":"2024-09-04T07:31:35.941678Z","shell.execute_reply":"2024-09-04T07:31:35.946122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training hyperparameters\nnum_epochs = 80\nbatch_size = 400 # Medium\nlearning_rate = 1e-3\n\n# Build training dataloader\nx = torch.from_numpy(train)\ntrain_dataset = CustomTensorDataset(x)\n\ntrain_sampler = RandomSampler(train_dataset)  # 随机采样数据\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n\n\n# 定义一个函数来创建MultiEncoderAutoencoder的实例，参数可以直接从这里修改，不再定义一个全局变量了\ndef create_multi_encoder_autoencoder(Split_channel=False, Multi_Type='fcn'):\n    return MultiEncoderAutoencoder(Split_channel=Split_channel, Multi_Type=Multi_Type)\n\n# Model\nmodel_type = 'Multi'   # selecting a model type from {'cnn', 'fcn', 'vae', 'resnet','Multi'}\nmodel_classes = {\n    'fcn': fcn_autoencoder(), \n    'cnn': conv_autoencoder(), \n    'vae': fcn_autoencoder(), \n    'resnet': ResNet(),\n    'Multi': create_multi_encoder_autoencoder\n    \n}\n\n# 创建模型实例并移动到GPU\nif model_type == 'Multi':\n    model = model_classes[model_type](Split_channel=False, Multi_Type='fcn').cuda()\nelse:\n    model = model_classes[model_type].cuda()\n\n\n# Loss and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)","metadata":{"id":"in7yLfmqtZTk","execution":{"iopub.status.busy":"2024-09-04T07:56:02.861209Z","iopub.execute_input":"2024-09-04T07:56:02.861569Z","iopub.status.idle":"2024-09-04T07:56:03.749670Z","shell.execute_reply.started":"2024-09-04T07:56:02.861544Z","shell.execute_reply":"2024-09-04T07:56:03.748771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training loop","metadata":{"id":"wyooN-JPm8sS"}},{"cell_type":"code","source":"# class test:\n#     def __init__(self, ic, oc):\n#         self.ic = ic\n#         self.oc = oc\n#         self.hh = [ic, oc]\n\n# hh = test(2, 3)\n# aq = *(hh.hh)\n# print(aq)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T07:26:37.389718Z","iopub.status.idle":"2024-09-04T07:26:37.390315Z","shell.execute_reply.started":"2024-09-04T07:26:37.390027Z","shell.execute_reply":"2024-09-04T07:26:37.390052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_loss = np.inf\n\nqqdm_train = qqdm(range(num_epochs), desc=format_str('bold', 'Description'))\nfor epoch in qqdm_train:\n    tot_loss = list() # 每个 epoch 开始时，tot_loss 被重新初始化为一个空列表\n    model.train()\n    for data in train_dataloader: # data是一个batch上的\n\n        # ===================loading=====================\n        img = data.float().cuda()\n        # img = data.float()\n        if model_type in ['fcn']:\n            img = img.view(img.shape[0], -1) # 全连接网络要将img展平为一维向量，从 (batch_size, height, width, channels) 转换为 (batch_size, height * width * channels)\n\n        # ===================forward=====================\n        output = model(img)\n        \n        if model_type in ['vae']:\n            loss = loss_vae(output[0], img, output[1], output[2], criterion) # output[0]：重建的图像。img：原始图像。output[1]：潜在变量的均值。output[2]：潜在变量的对数方差。criterion：损失函数\n        else:\n            loss = criterion(output, img)\n\n        tot_loss.append(loss.item()) # tot_loss是一个list, 用于存储当前 epoch 中所有批次的损失值\n        \n        # ===================backward====================\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    # ===================save_best====================\n    mean_loss = np.mean(tot_loss)  # 当前epoch上的平均损失\n    if mean_loss < best_loss:\n        best_loss = mean_loss\n        torch.save(model, 'best_model_{}.pt'.format(model_type))\n        \n    # ===================log========================\n    qqdm_train.set_infos({\n        'epoch': f'{epoch + 1:.0f}/{num_epochs:.0f}',\n        'loss':f'{mean_loss:.5f}'\n    })\n    \n    # ===================save_last========================\n    torch.save(model, 'last_model_{}.pt'.format(model_type))","metadata":{"id":"JoW1UrrxgI_U","execution":{"iopub.status.busy":"2024-09-04T07:56:04.783815Z","iopub.execute_input":"2024-09-04T07:56:04.784166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\nModel is loaded and generates its anomaly score predictions.","metadata":{"id":"Wk0UxFuchLzR"}},{"cell_type":"markdown","source":"## Initialize\n- dataloader\n- model\n- prediction file","metadata":{"id":"evgMW3OwoGqD"}},{"cell_type":"code","source":"eval_batch_size = 200\n\n# build testing dataloader\ndata = torch.tensor(test, dtype=torch.float32)\ntest_dataset = CustomTensorDataset(data)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=eval_batch_size, num_workers=1) # 也可以用shuffle=True代替sampler\neval_loss = nn.MSELoss(reduction='none')\n\n# load trained model\ncheckpoint_path = f'last_model_{model_type}.pt'\nmodel = torch.load(checkpoint_path)\nmodel.eval()\n\n# prediction file\nout_file = 'prediction.csv'","metadata":{"id":"_MBnXAswoKmq","execution":{"iopub.status.busy":"2024-09-04T07:26:37.394999Z","iopub.status.idle":"2024-09-04T07:26:37.395387Z","shell.execute_reply.started":"2024-09-04T07:26:37.395197Z","shell.execute_reply":"2024-09-04T07:26:37.395216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#see picture reconstructed\nimport matplotlib.pyplot as plt\n\n# 准备输入数据\nreal_data = train_dataset[:3]  # 从训练集选择前三张图片\nreal_data = real_data.cuda()\n# print(f\"input data shape: {real_data.size()}\")\nif model_type in ['fcn']:\n    input_data = real_data.reshape(real_data.size(0), -1)   # 展平输入数据\nelse:\n    input_data = real_data\n# 编码和解码\nwith torch.no_grad():\n    decoded_data= model(input_data)\ndecoded_data = decoded_data.float().cuda()\nprint(f\"output data shape: {decoded_data.size()}\")\n\nif model_type in ['fcn']:\n    decoded_data = decoded_data.reshape(real_data.size(0), 3, 64, 64)  # 重塑输出数据\nelif model_type in ['vae']:\n    decoded_data = decoded_data[0]\n#print(f\"output data shape: {decoded_data.size()}\")\n\n# 创建画布\nfig, axes = plt.subplots(nrows=2, ncols=len(input_data), figsize=(5, 5)) # 两行三列子图\n\n# 显示原始图像\nfor idx in range(len(real_data)):\n    ax = axes[0][idx]\n    ax.imshow(transforms.ToPILImage()((real_data[idx]+1)/2)) # ToPILImage输入是[0,1]\n    ax.axis('off')\n    ax.annotate('Original Image', xy=(0.5, -0.15), xycoords='axes fraction',ha='center', va='center')\n    \n# 显示重构图像\nfor idx in range(len(decoded_data)):\n    ax = axes[1][idx]\n    ax.imshow(transforms.ToPILImage()((decoded_data[idx]+1)/2))\n    ax.axis('off')\n    ax.annotate('Model Output', xy=(0.5, -0.15), xycoords='axes fraction',ha='center', va='center')\n    \nplt.tight_layout()  # 调整子图之间的间距，防止文本被截断\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-04T07:26:37.396634Z","iopub.status.idle":"2024-09-04T07:26:37.397003Z","shell.execute_reply.started":"2024-09-04T07:26:37.396818Z","shell.execute_reply":"2024-09-04T07:26:37.396841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 2   \nTrain a fully connected autoencoder and adjust at least two different element of the latent representation. Show your model architecture, plot out the original image, the reconstructed images for each adjustment and describe the differences.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport matplotlib.pyplot as plt\n# sample = train_dataset[random.randint(0,100000)]\nsample = train_dataset[0]\nprint(\"sample shape:{}\".format(sample.size()))\nsample = sample.reshape(1,3,64,64)\n\nmodel.eval()\nwith torch.no_grad():\n    img = sample.cuda()\n            \n    # 只调整fcn中的latent representation的其中两维，其他模型都是正常输出\n    if model_type in ['res']:\n        output = model(img)\n        output = decoder(output)\n        print(\"res output shape:{}\".format(output.size()))\n        output = output[0] # 第一个重建图像，当然只有一个图像\n        \n    if model_type in ['fcn']:\n        img = img.reshape(img.shape[0], -1)\n        x = model.encoder(img)\n        x[0][2] = x[0][2]*3\n        output = model.decoder(x)\n        print(\"fcn output shape:{}\".format(output.size()))\n        output = output.reshape(3,64,64)\n        \n    if model_type in ['vae']:\n        output = model(img)\n        print(\"vae output shape:{}\".format(output.size()))\n        output = output[0][0] # output[0]是重建后的图像，output[0][0]重建后的第一个图像\n        \n    if model_type in ['cnn']:\n        output = model(img)[0]\n        \n    print(\"output shape:{}\".format(output.size()))\n       \nsample = sample.reshape(3,64,64)   \n\n# 创建画布\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(5, 5))\n\n# plt sample image\naxes[0].imshow(transforms.ToPILImage()((sample+1)/2)) #imshow的输入(H,W,C)\naxes[0].axis('off')\naxes[0].annotate('sample input', xy=(0.5, -0.15), xycoords='axes fraction',ha='center', va='center')\n# plt output image\naxes[1].imshow(transforms.ToPILImage()((output+1)/2))\naxes[1].axis('off')\naxes[1].annotate('sample output', xy=(0.5, -0.15), xycoords='axes fraction',ha='center', va='center')\n\nplt.show()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-09-04T07:26:37.397835Z","iopub.status.idle":"2024-09-04T07:26:37.398169Z","shell.execute_reply.started":"2024-09-04T07:26:37.398006Z","shell.execute_reply":"2024-09-04T07:26:37.398019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anomality = list()\nwith torch.no_grad():\n  for i, data in enumerate(test_dataloader):\n    img = data.float().cuda()\n    if model_type in ['fcn']:\n      img = img.view(img.shape[0], -1)\n    output = model(img)\n    if model_type in ['vae']:\n      output = output[0]\n    if model_type in ['fcn']:\n        loss = eval_loss(output, img).sum(-1)\n    else:\n        loss = eval_loss(output, img).sum([1, 2, 3]) # 在123维度上求和\n        \n    anomality.append(loss) # 多个批次异常检测结果的列表\n    \nanomality = torch.cat(anomality, axis=0) # 在第0维度（批次维度）上拼接异常检测的结果，合并成一个大的张量。\nanomality = torch.sqrt(anomality).reshape(len(test), 1).cpu().numpy() # 将张量重塑为形状 (len(test), 1)\n\ndf = pd.DataFrame(anomality, columns=['score'])\ndf.to_csv(out_file, index_label = 'ID')","metadata":{"id":"_1IxCX2iCW6V","execution":{"iopub.status.busy":"2024-09-04T07:26:37.400565Z","iopub.status.idle":"2024-09-04T07:26:37.400916Z","shell.execute_reply.started":"2024-09-04T07:26:37.400746Z","shell.execute_reply":"2024-09-04T07:26:37.400760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}